# ============================================================================
# OpenObserve Pipeline Template - Production Reference Guide
# ============================================================================
# This template is based on working, tested pipeline configurations
# All examples have been verified to work with the o2operator
#
# Version: v1alpha1
# Last Updated: 2024-12-01
# ============================================================================

apiVersion: openobserve.ai/v1alpha1
kind: OpenObservePipeline
metadata:
  name: <pipeline-name>        # Kubernetes resource name (lowercase, hyphens)
  namespace: o2operator         # Namespace where pipeline resource is created
spec:
  # ============================================================================
  # CONFIGURATION REFERENCE (Required)
  # Links this pipeline to an OpenObserve instance
  # ============================================================================
  configRef:
    name: openobserve-main      # Name of the OpenObserveConfig resource (Required)
    namespace: o2operator       # Namespace of the config (Optional, defaults to pipeline namespace)

  # ============================================================================
  # PIPELINE IDENTIFICATION
  # ============================================================================

  # Pipeline name in OpenObserve (Required)
  # This is the internal name in OpenObserve, can be same as K8s resource name
  name: "pipeline-name"

  # Pipeline description (Optional)
  description: "Description of what this pipeline does"

  # Enable/disable the pipeline (Optional, defaults to true)
  enabled: true

  # Organization name (Optional, usually defaults to "default")
  org: "default"

  # Pipeline ID (Optional - auto-generated by OpenObserve if not provided)
  # pipeline_id: "7400591410606374912"

  # ============================================================================
  # SOURCE CONFIGURATION (Required)
  # Defines the input for the pipeline
  # ============================================================================

  # For REALTIME pipelines (process data as it arrives):
  source:
    streamName: "application_logs"   # Source stream name (Required for realtime)
    streamType: "logs"               # Type: logs, metrics, or traces
    sourceType: "realtime"           # Process data as it arrives

  # For SCHEDULED pipelines (batch processing with queries):
  # source:
  #   sourceType: "scheduled"         # Run on a schedule
  #   streamType: "logs"              # Type of data being queried
  #   queryCondition:                 # Query configuration
  #     type: "sql"                   # Query type: sql or promql
  #     sql: "SELECT * FROM default"  # SQL query
  #     searchEventType: "derivedstream"
  #   triggerCondition:               # Schedule configuration
  #     period: 1
  #     operator: "="
  #     threshold: 0
  #     frequency: 1
  #     frequencyType: "minutes"      # minutes, hours, days
  #     silence: 0
  #     timezone: "UTC"
  #     alignTime: true

  # ============================================================================
  # PIPELINE NODES (Required - at least 1 node)
  # Define processing steps
  # ============================================================================
  nodes:
    # ----------------------------------
    # NODE TYPE: CONDITION (Filtering)
    # ----------------------------------
    - id: "filter-node"              # Unique ID within pipeline
      type: "condition"              # Node type
      name: "Filter by Level"        # Display name
      config:
        conditions:                  # Filter conditions
          - column: "level"
            operator: "="            # Operators: =, !=, >, <, >=, <=, contains, not_contains
            value: "info"
            ignore_case: true

    # ----------------------------------
    # NODE TYPE: FUNCTION (Transform)
    # ----------------------------------
    - id: "transform-node"
      type: "function"
      name: "Transform Data"
      config:
        function: |                 # VRL function or function name
          .processed_at = now()
          .environment = "production"
        after_flatten: false         # Execute after flattening
        num_args: 0                  # Number of arguments

    # ----------------------------------
    # NODE TYPE: STREAM (Output)
    # ----------------------------------
    - id: "output-stream"
      type: "stream"
      name: "Output to Stream"
      config:
        stream_name: "processed_logs"   # Target stream name
        stream_type: "logs"             # Target stream type

    # ----------------------------------
    # NODE TYPE: REMOTE STREAM (External)
    # ----------------------------------
    - id: "remote-output"
      type: "remote_stream"
      name: "Send to Remote"
      config:
        destination_name: "openobserve"  # Remote destination

  # ============================================================================
  # PIPELINE EDGES (Optional)
  # Define connections between nodes
  # ============================================================================
  edges:
    - source: "filter-node"
      target: "transform-node"
    - source: "transform-node"
      target: "output-stream"

  # Note: If edges are empty [], the pipeline automatically connects:
  # source -> first node -> subsequent nodes in order

# ============================================================================
# WORKING EXAMPLES
# ============================================================================

---
# Example 1: Simple Source to Destination
apiVersion: openobserve.ai/v1alpha1
kind: OpenObservePipeline
metadata:
  name: simple-pipeline
  namespace: o2operator
spec:
  configRef:
    name: openobserve-main

  name: "simple-pipeline"
  description: "Direct stream routing"
  enabled: true
  org: "default"

  source:
    streamName: "application_logs"
    streamType: "logs"
    sourceType: "realtime"

  nodes:
    - id: "output"
      type: "stream"
      name: "Route to Output"
      config:
        stream_name: "routed_logs"
        stream_type: "logs"

  edges: []  # Direct connection from source to output

---
# Example 2: Filter and Branch Pipeline
apiVersion: openobserve.ai/v1alpha1
kind: OpenObservePipeline
metadata:
  name: multi-branch-pipeline
  namespace: o2operator
spec:
  configRef:
    name: openobserve-main

  name: "multi-branch-pipeline"
  description: "Filter and route to multiple destinations"
  enabled: true

  source:
    streamName: "application_logs"
    streamType: "logs"
    sourceType: "realtime"

  nodes:
    # Filter for errors
    - id: "error-filter"
      type: "condition"
      name: "Filter Errors"
      config:
        conditions:
          - column: "level"
            operator: "="
            value: "error"
            ignore_case: true

    # Filter for warnings
    - id: "warn-filter"
      type: "condition"
      name: "Filter Warnings"
      config:
        conditions:
          - column: "level"
            operator: "="
            value: "warning"
            ignore_case: true

    # Transform errors
    - id: "error-transform"
      type: "function"
      name: "Add Error Metadata"
      config:
        function: |
          .alert_priority = "high"
          .requires_action = true
        after_flatten: false
        num_args: 0

    # Output for errors
    - id: "error-output"
      type: "stream"
      name: "Error Stream"
      config:
        stream_name: "critical_errors"
        stream_type: "logs"

    # Output for warnings
    - id: "warn-output"
      type: "stream"
      name: "Warning Stream"
      config:
        stream_name: "warnings"
        stream_type: "logs"

  edges:
    # Error path
    - source: "error-filter"
      target: "error-transform"
    - source: "error-transform"
      target: "error-output"
    # Warning path
    - source: "warn-filter"
      target: "warn-output"

---
# Example 3: Scheduled SQL Query Pipeline
apiVersion: openobserve.ai/v1alpha1
kind: OpenObservePipeline
metadata:
  name: hourly-aggregation
  namespace: o2operator
spec:
  configRef:
    name: openobserve-main

  name: "hourly-aggregation"
  description: "Hourly log aggregation"
  enabled: true

  source:
    sourceType: "scheduled"
    streamType: "logs"
    queryCondition:
      type: "sql"
      sql: |
        SELECT
          level,
          COUNT(*) as count,
          DATE_TRUNC('hour', _timestamp) as hour
        FROM default
        WHERE _timestamp >= NOW() - INTERVAL '1 hour'
        GROUP BY level, hour
      searchEventType: "derivedstream"
    triggerCondition:
      period: 60
      operator: ">"
      threshold: 0
      frequency: 60
      frequencyType: "minutes"
      silence: 0
      timezone: "UTC"
      alignTime: true

  nodes:
    - id: "output"
      type: "stream"
      name: "Aggregation Output"
      config:
        stream_name: "hourly_stats"
        stream_type: "logs"

  edges: []

---
# Example 4: PromQL Query Pipeline
apiVersion: openobserve.ai/v1alpha1
kind: OpenObservePipeline
metadata:
  name: metrics-pipeline
  namespace: o2operator
spec:
  configRef:
    name: openobserve-main

  name: "metrics-pipeline"
  description: "Process metrics with PromQL"
  enabled: true

  source:
    sourceType: "scheduled"
    streamType: "metrics"
    queryCondition:
      type: "promql"
      promql: "rate(http_requests_total[5m])"
      promqlCondition:
        column: "value"
        operator: ">"
        value: 100
        ignoreCase: false
      searchEventType: "derivedstream"
    triggerCondition:
      period: 5
      operator: ">"
      threshold: 0
      frequency: 5
      frequencyType: "minutes"
      silence: 0
      timezone: "UTC"
      alignTime: true

  nodes:
    - id: "output"
      type: "stream"
      name: "High Rate Alerts"
      config:
        stream_name: "high_request_rates"
        stream_type: "logs"

  edges: []

---
# Example 5: Complex Processing Pipeline
apiVersion: openobserve.ai/v1alpha1
kind: OpenObservePipeline
metadata:
  name: complex-pipeline
  namespace: o2operator
spec:
  configRef:
    name: openobserve-main

  name: "complex-pipeline"
  description: "Multi-stage processing with conditions and transforms"
  enabled: true

  source:
    streamName: "raw_logs"
    streamType: "logs"
    sourceType: "realtime"

  nodes:
    # Stage 1: Parse JSON
    - id: "parse"
      type: "function"
      name: "Parse JSON"
      config:
        function: |
          if exists(.message) && starts_with(.message, "{") {
            .parsed = parse_json!(.message)
            . = merge!(., .parsed)
            del(.message)
          }
        after_flatten: false
        num_args: 0

    # Stage 2: Filter by level
    - id: "level-filter"
      type: "condition"
      name: "Filter Important"
      config:
        conditions:
          - column: "level"
            operator: "="
            value: "error"
            ignore_case: true

    # Stage 3: Enrich data
    - id: "enrich"
      type: "function"
      name: "Add Metadata"
      config:
        function: |
          .timestamp = now()
          .cluster = "production"
          .processed_by = "o2operator"
        after_flatten: false
        num_args: 0

    # Stage 4: Output to multiple destinations
    - id: "primary-output"
      type: "stream"
      name: "Primary Storage"
      config:
        stream_name: "processed_errors"
        stream_type: "logs"

    - id: "backup-output"
      type: "remote_stream"
      name: "Backup Storage"
      config:
        destination_name: "openobserve"

  edges:
    # Processing chain
    - source: "parse"
      target: "level-filter"
    - source: "level-filter"
      target: "enrich"
    - source: "enrich"
      target: "primary-output"
    - source: "enrich"
      target: "backup-output"

# ============================================================================
# SUPPORTED NODE TYPES
# ============================================================================
#
# 1. condition     - Filter data based on conditions
# 2. function      - Transform data using VRL functions
# 3. stream        - Output to OpenObserve stream
# 4. remote_stream - Output to remote destination
# 5. query         - Query data (for scheduled pipelines)
#
# ============================================================================
# SUPPORTED OPERATORS FOR CONDITIONS
# ============================================================================
#
# - "="            Equal
# - "!="           Not equal
# - ">"            Greater than
# - "<"            Less than
# - ">="           Greater than or equal
# - "<="           Less than or equal
# - "contains"     String contains
# - "not_contains" String does not contain
#
# ============================================================================
# TROUBLESHOOTING
# ============================================================================
#
# 1. Pipeline not processing data:
#    - Check if enabled: true
#    - Verify source stream exists and has data
#    - Check conditions aren't too restrictive
#
# 2. Scheduled pipeline not running:
#    - Verify triggerCondition settings
#    - Check queryCondition syntax
#    - Ensure sourceType is "scheduled"
#
# 3. Data not reaching output:
#    - Verify edges connect all nodes
#    - Check output stream exists
#    - Review function syntax for errors
#
# 4. Function errors:
#    - Test VRL syntax separately
#    - Check field existence: exists(.field)
#    - Use error handling: parse_json!() vs parse_json()
#
# ============================================================================